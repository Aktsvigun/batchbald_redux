{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp active_learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appended /home/blackhc/PycharmProjects/blackhc.batchbald/src to paths\n",
      "Switched to directory /home/blackhc/PycharmProjects/blackhc.batchbald\n",
      "%load_ext autoreload\n",
      "%autoreload 2\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "import blackhc.project.script\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Active learning\n",
    "> Everything needed for active Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Active Learning Data\n",
    "\n",
    "For active learning, we need to split the available training data between a training set and a pool set of (unlabelled) data, which we score using our model and acquisition function and add to the training set peu a peu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "from typing import Dict, List\n",
    "import numpy as np\n",
    "import torch.utils.data as data\n",
    "import torch\n",
    "import collections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "\n",
    "class ActiveLearningData:\n",
    "    \"\"\"Splits `dataset` into an active dataset and an available dataset.\"\"\"\n",
    "    active_dataset: data.Dataset\n",
    "    available_dataset: data.Dataset\n",
    "\n",
    "    def __init__(self, dataset: data.Dataset):\n",
    "        super().__init__()\n",
    "        self.dataset = dataset\n",
    "        self.active_mask = np.full((len(dataset),), False)\n",
    "        self.available_mask = np.full((len(dataset),), True)\n",
    "\n",
    "        self.active_dataset = data.Subset(self.dataset, None)\n",
    "        self.available_dataset = data.Subset(self.dataset, None)\n",
    "\n",
    "        self._update_indices()\n",
    "\n",
    "    def _update_indices(self):\n",
    "        self.active_dataset.indices = np.nonzero(self.active_mask)[0]\n",
    "        self.available_dataset.indices = np.nonzero(self.available_mask)[0]\n",
    "\n",
    "    def get_dataset_indices(self, available_indices: List[int]) -> List[int]:\n",
    "        indices = self.available_dataset.indices[available_indices]\n",
    "        return indices\n",
    "\n",
    "    def acquire(self, available_indices):\n",
    "        indices = self.get_dataset_indices(available_indices)\n",
    "\n",
    "        self.active_mask[indices] = True\n",
    "        self.available_mask[indices] = False\n",
    "        self._update_indices()\n",
    "\n",
    "    def make_unavailable(self, available_indices):\n",
    "        indices = self.get_dataset_indices(available_indices)\n",
    "\n",
    "        self.available_mask[indices] = False\n",
    "        self._update_indices()\n",
    "\n",
    "    def get_random_available_indices(self, size) -> torch.LongTensor:\n",
    "        assert 0 <= size <= len(self.available_dataset)\n",
    "        available_indices = torch.randperm(len(self.available_dataset))[:size]\n",
    "        return available_indices\n",
    "\n",
    "    def extract_dataset(self, size) -> data.Dataset:\n",
    "        \"\"\"Extract a dataset randomly from the available dataset and make those indices unavailable.\n",
    "        \n",
    "        Useful for extracting a validation set.\"\"\"\n",
    "        return self.extract_dataset_from_indices(self.get_random_available_indices(size))\n",
    "\n",
    "    def extract_dataset_from_indices(self, available_indices) -> data.Dataset:\n",
    "        \"\"\"Extract a dataset from the available dataset and make those indices unavailable.\n",
    "        \n",
    "        Useful for extracting a validation set.\"\"\"\n",
    "        dataset_indices = self.get_dataset_indices(available_indices)\n",
    "\n",
    "        self.make_unavailable(available_indices)\n",
    "        return data.Subset(self.dataset, dataset_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"ActiveLearningData.get_dataset_indices\" class=\"doc_header\"><code>ActiveLearningData.get_dataset_indices</code><a href=\"__main__.py#L23\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>ActiveLearningData.get_dataset_indices</code>(**`available_indices`**:`List`\\[`int`\\])\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"ActiveLearningData.acquire\" class=\"doc_header\"><code>ActiveLearningData.acquire</code><a href=\"__main__.py#L27\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>ActiveLearningData.acquire</code>(**`available_indices`**)\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"ActiveLearningData.make_unavailable\" class=\"doc_header\"><code>ActiveLearningData.make_unavailable</code><a href=\"__main__.py#L34\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>ActiveLearningData.make_unavailable</code>(**`available_indices`**)\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"ActiveLearningData.get_random_available_indices\" class=\"doc_header\"><code>ActiveLearningData.get_random_available_indices</code><a href=\"__main__.py#L40\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>ActiveLearningData.get_random_available_indices</code>(**`size`**)\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"ActiveLearningData.extract_dataset\" class=\"doc_header\"><code>ActiveLearningData.extract_dataset</code><a href=\"__main__.py#L45\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>ActiveLearningData.extract_dataset</code>(**`size`**)\n",
       "\n",
       "Extract a dataset randomly from the available dataset and make those indices unavailable.\n",
       "\n",
       "Useful for extracting a validation set."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"ActiveLearningData.extract_dataset_from_indices\" class=\"doc_header\"><code>ActiveLearningData.extract_dataset_from_indices</code><a href=\"__main__.py#L51\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>ActiveLearningData.extract_dataset_from_indices</code>(**`available_indices`**)\n",
       "\n",
       "Extract a dataset from the available dataset and make those indices unavailable.\n",
       "\n",
       "Useful for extracting a validation set."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(ActiveLearningData.get_dataset_indices)\n",
    "show_doc(ActiveLearningData.acquire)\n",
    "show_doc(ActiveLearningData.make_unavailable)\n",
    "show_doc(ActiveLearningData.get_random_available_indices)\n",
    "show_doc(ActiveLearningData.extract_dataset)\n",
    "show_doc(ActiveLearningData.extract_dataset_from_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "\n",
    "def get_balanced_sample_indices(target_classes: List, num_classes, n_per_digit=2) -> List[int]:\n",
    "    permed_indices = torch.randperm(len(target_classes))\n",
    "\n",
    "    if n_per_digit == 0:\n",
    "        return []\n",
    "    \n",
    "    num_samples_by_class = collections.defaultdict(int)\n",
    "    initial_samples = []\n",
    "    \n",
    "    for i in range(len(permed_indices)):\n",
    "        permed_index = int(permed_indices[i])\n",
    "        index, target = permed_index, int(target_classes[permed_index])\n",
    "\n",
    "        num_target_samples = num_samples_by_class[target]\n",
    "        if num_target_samples == n_per_digit:\n",
    "            continue\n",
    "            \n",
    "        initial_samples.append(index)        \n",
    "        num_samples_by_class[target] += 1\n",
    "\n",
    "        if len(initial_samples) == num_classes * n_per_digit:\n",
    "            break\n",
    "\n",
    "    return initial_samples\n",
    "\n",
    "def get_subset_base_indices(dataset: data.Subset, indices: List[int]):\n",
    "    return [int(dataset.indices[index]) for index in indices]\n",
    "\n",
    "\n",
    "def get_base_indices(dataset: data.Dataset, indices: List[int]):\n",
    "    if isinstance(dataset, data.Subset):\n",
    "        return get_base_indices(dataset.dataset, get_subset_base_indices(dataset, indices))\n",
    "    return indices\n",
    "\n",
    "\n",
    "class RandomFixedLengthSampler(data.Sampler):\n",
    "    \"\"\"\n",
    "    Sometimes, you really want to do more with little data without increasing the number of epochs.\n",
    "\n",
    "    This sampler takes a `dataset` and draws `target_length` samples from it (with repetition).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset: data.Dataset, target_length):\n",
    "        super().__init__(dataset)\n",
    "        self.dataset = dataset\n",
    "        self.target_length = target_length\n",
    "\n",
    "    def __iter__(self):\n",
    "        # Ensure that we don't lose data by accident.\n",
    "        if self.target_length < len(self.dataset):\n",
    "            return iter(range(len(self.dataset)))\n",
    "\n",
    "        return iter((torch.randperm(self.target_length) % len(self.dataset)).tolist())\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.target_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

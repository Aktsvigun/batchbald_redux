---

title: Consistent MC Dropout

keywords: fastai
sidebar: home_sidebar

summary: "Custom consistent dropout modules"
description: "Custom consistent dropout modules"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: consistent_mc_dropout.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>For BNNs, we are going to use MC dropout.</p>
<p>To be able to compute BatchBALD scores, we need consistent MC dropout, which uses the consistent masks for inference. That means, that we draw $K$ masks and then use them to draw the $K$ samples.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Bayesian-Module">Bayesian Module<a class="anchor-link" href="#Bayesian-Module"> </a></h2><p>To make this work in an efficient way, we are going to define an abstract wrapper module that takes a batch <code>input_B</code> and outputs <code>results_B_K</code>.</p>
<p>Internally, it will pass the batch through <code>deterministic_forward_impl</code>, which can be overridden, and then blow it up to $(B \cdot K) \times \cdots$ and then pass it to <code>mc_forward_impl</code>, which can be overriden.</p>
<p><a href="/batchbald_redux/consistent_mc_dropout#ConsistentMCDropout"><code>ConsistentMCDropout</code></a> layers will know to reshape the inputs to $B \times K \times \cdots$ and apply consistent masks.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="BayesianModule" class="doc_header"><code>class</code> <code>BayesianModule</code><a href="https://github.com/blackhc/batchbald_redux/tree/master/batchbald_redux/consistent_mc_dropout.py#L12" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>BayesianModule</code>() :: <code>Module</code></p>
</blockquote>
<p>A module that we can sample multiple times from given a single input batch.</p>
<p>To be efficient, the module allows for a part of the forward pass to be deterministic.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Consistent-MC-Dropout">Consistent MC Dropout<a class="anchor-link" href="#Consistent-MC-Dropout"> </a></h2>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="ConsistentMCDropout" class="doc_header"><code>class</code> <code>ConsistentMCDropout</code><a href="https://github.com/blackhc/batchbald_redux/tree/master/batchbald_redux/consistent_mc_dropout.py#L118" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>ConsistentMCDropout</code>(<strong><code>p</code></strong>=<em><code>0.5</code></em>) :: <code>_ConsistentMCDropout</code></p>
</blockquote>
<p>Randomly zeroes some of the elements of the input
tensor with probability :attr:<code>p</code> using samples from a Bernoulli
distribution. The elements to zero are randomized on every forward call during training time.</p>
<p>During eval time, a fixed mask is picked and kept until <code>reset_mask()</code> is called.</p>
<p>This has proven to be an effective technique for regularization and
preventing the co-adaptation of neurons as described in the paper
<code>Improving neural networks by preventing co-adaptation of feature
detectors</code>_ .</p>
<p>Furthermore, the outputs are scaled by a factor of :math:<code>\frac{1}{1-p}</code> during
training. This means that during evaluation the module simply computes an
identity function.</p>
<p>Args:
    p: probability of an element to be zeroed. Default: 0.5
    inplace: If set to <code>True</code>, will do this operation in-place. Default: <code>False</code></p>
<p>Shape:</p>

<pre><code>- Input: `Any`. Input can be of any shape
- Output: `Same`. Output is of the same shape as input

</code></pre>
<p>Examples::</p>

<pre><code>&gt;&gt;&gt; m = nn.Dropout(p=0.2)
&gt;&gt;&gt; input = torch.randn(20, 16)
&gt;&gt;&gt; output = m(input)

</code></pre>
<p>.. _Improving neural networks by preventing co-adaptation of feature
    detectors: <a href="https://arxiv.org/abs/1207.0580">https://arxiv.org/abs/1207.0580</a></p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="ConsistentMCDropout2d" class="doc_header"><code>class</code> <code>ConsistentMCDropout2d</code><a href="https://github.com/blackhc/batchbald_redux/tree/master/batchbald_redux/consistent_mc_dropout.py#L154" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>ConsistentMCDropout2d</code>(<strong><code>p</code></strong>=<em><code>0.5</code></em>) :: <code>_ConsistentMCDropout</code></p>
</blockquote>
<p>Randomly zeroes whole channels of the input tensor.
The channels to zero-out are randomized on every forward call.</p>
<p>During eval time, a fixed mask is picked and kept until <code>reset_mask()</code> is called.</p>
<p>Usually the input comes from :class:<code>nn.Conv2d</code> modules.</p>
<p>As described in the paper
<code>Efficient Object Localization Using Convolutional Networks</code>_ ,
if adjacent pixels within feature maps are strongly correlated
(as is normally the case in early convolution layers) then i.i.d. dropout
will not regularize the activations and will otherwise just result
in an effective learning rate decrease.</p>
<p>In this case, :func:<code>nn.Dropout2d</code> will help promote independence between
feature maps and should be used instead.</p>
<p>Args:
    p (float, optional): probability of an element to be zero-ed.
    inplace (bool, optional): If set to <code>True</code>, will do this operation
        in-place</p>
<p>Shape:</p>

<pre><code>- Input: :math:`(N, C, H, W)`
- Output: :math:`(N, C, H, W)` (same shape as input)

</code></pre>
<p>Examples::</p>

<pre><code>&gt;&gt;&gt; m = nn.Dropout2d(p=0.2)
&gt;&gt;&gt; input = torch.randn(20, 16, 32, 32)
&gt;&gt;&gt; output = m(input)

</code></pre>
<p>.. _Efficient Object Localization Using Convolutional Networks:
   <a href="http://arxiv.org/abs/1411.4280">http://arxiv.org/abs/1411.4280</a></p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

</div>
 


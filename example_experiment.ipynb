{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Experiment\n",
    "> Experiment using Repeated MNIST and BatchBALD vs BALD vs random sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook ties everything together and runs an AL loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appended /home/blackhc/PycharmProjects/blackhc.batchbald/src to paths\n",
      "Switched to directory /home/blackhc/PycharmProjects/blackhc.batchbald\n",
      "%load_ext autoreload\n",
      "%autoreload 2\n"
     ]
    }
   ],
   "source": [
    "import blackhc.project.script\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from batchbald_redux import active_learning, batchbald, consistent_mc_dropout, joint_entropy, repeated_mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define our Bayesian CNN model that we will use to train MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesianCNN(consistent_mc_dropout.BayesianModule):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=5)\n",
    "        self.conv1_drop = consistent_mc_dropout.ConsistentMCDropout2d()\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=5)\n",
    "        self.conv2_drop = consistent_mc_dropout.ConsistentMCDropout2d()\n",
    "        self.fc1 = nn.Linear(1024, 128)\n",
    "        self.fc1_drop = consistent_mc_dropout.ConsistentMCDropout()\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def mc_forward_impl(self, input: torch.Tensor):\n",
    "        input = F.relu(F.max_pool2d(self.conv1_drop(self.conv1(input)), 2))\n",
    "        input = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(input)), 2))\n",
    "        input = input.view(-1, 1024)\n",
    "        input = F.relu(self.fc1_drop(self.fc1(input)))\n",
    "        input = self.fc2(input)\n",
    "        input = F.log_softmax(input, dim=1)\n",
    "\n",
    "        return input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grab our dataset, we'll use Repeated-MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = repeated_mnist.create_repeated_MNIST_dataset(num_repetitions=1, add_noise=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_initial_samples = 20\n",
    "num_classes = 10\n",
    "\n",
    "active_learning_data = active_learning.ActiveLearningData(train_dataset)\n",
    "\n",
    "initial_samples = active_learning.get_balanced_sample_indices(\n",
    "    repeated_mnist.get_targets(train_dataset),\n",
    "    num_classes=num_classes,\n",
    "    n_per_digit=2)\n",
    "\n",
    "# Split off the initial samples first.\n",
    "active_learning_data.acquire(initial_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch_size = 512\n",
    "batch_size = 64\n",
    "scoring_batch_size = 512\n",
    "epoch_samples = 8192*4\n",
    "\n",
    "use_cuda = False\n",
    "\n",
    "kwargs = {\"num_workers\": 1, \"pin_memory\": True} if use_cuda else {}\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                          batch_size=test_batch_size,\n",
    "                                          shuffle=False,\n",
    "                                          **kwargs)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    active_learning_data.active_dataset,\n",
    "    sampler=active_learning.RandomFixedLengthSampler(active_learning_data.active_dataset,\n",
    "                                     epoch_samples),\n",
    "    batch_size=batch_size,\n",
    "    **kwargs,\n",
    ")\n",
    "\n",
    "available_loader = torch.utils.data.DataLoader(\n",
    "    active_learning_data.available_dataset,\n",
    "    batch_size=scoring_batch_size,\n",
    "    shuffle=False,\n",
    "    **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_active_samples = 150\n",
    "acquisition_batch_size = 10\n",
    "num_inference_samples = 20\n",
    "\n",
    "test_accs = []\n",
    "test_loss = []\n",
    "added_indices = []\n",
    "\n",
    "while True:\n",
    "    print(f\"Num acquired samples: {len(active_learning_data.active_dataset)}\")\n",
    "\n",
    "    model = BayesianCNN(num_classes)\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    # Train\n",
    "    for data, target in tqdm(train_loader, desc=\"Training\"):\n",
    "        #data = data.cuda()\n",
    "        #target = target.cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        prediction = model(data, 1).squeeze(1)\n",
    "        loss = F.nll_loss(prediction, target)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Test\n",
    "    loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in tqdm(test_loader, desc=\"Testing\"):\n",
    "            #data = data.cuda()\n",
    "            #target = target.cuda()\n",
    "\n",
    "            prediction = model(data, 1).squeeze(1)\n",
    "            loss += F.nll_loss(prediction, target, reduction=\"sum\")\n",
    "\n",
    "            prediction = prediction.max(1)[1]\n",
    "            correct += prediction.eq(target.view_as(prediction)).sum().item()\n",
    "\n",
    "    loss /= len(test_loader.dataset)\n",
    "    test_loss.append(loss)\n",
    "\n",
    "    percentage_correct = 100.0 * correct / len(test_loader.dataset)\n",
    "    test_accs.append(percentage_correct)\n",
    "\n",
    "    print(\"Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\".format(\n",
    "        loss, correct, len(test_loader.dataset), percentage_correct))\n",
    "\n",
    "    if len(active_learning_data.active_dataset) >= max_active_samples:\n",
    "        break\n",
    "\n",
    "    # Acquire pool predictions\n",
    "    N = len(active_learning_data.available_dataset)\n",
    "    probs_N_K_C = torch.empty((N, num_inference_samples, num_classes))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "\n",
    "        for i, (data, _) in enumerate(\n",
    "                tqdm(available_loader, desc=\"Evaluating Acquisition Set\")):\n",
    "            lower = i * available_loader.batch_size\n",
    "            upper = min(lower + available_loader.batch_size, N)\n",
    "            probs_N_K_C[lower:upper] = model(\n",
    "                data, num_inference_samples).double().exp_()\n",
    "\n",
    "    candidate_batch = batchbald.get_batchbald_batch(probs_N_K_C,\n",
    "                                                    acquisition_batch_size,\n",
    "                                                    dtype=torch.double)\n",
    "\n",
    "    targets = repeated_mnist.get_targets(\n",
    "        active_learning_data.available_dataset)\n",
    "    dataset_indices = active_learning_data.get_dataset_indices(\n",
    "        candidate_batch.indices)\n",
    "\n",
    "    print(\"Dataset indices: \", dataset_indices)\n",
    "    print(\"Scores: \", candidate_batch.scores)\n",
    "    print(\"Labels: \", targets[candidate_batch.indices])\n",
    "\n",
    "    active_learning_data.acquire(candidate_batch.indices)\n",
    "    added_indices.append(dataset_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

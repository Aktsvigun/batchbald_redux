{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp batchbald"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appended /home/blackhc/PycharmProjects/blackhc.batchbald/src to paths\n",
      "Switched to directory /home/blackhc/PycharmProjects/blackhc.batchbald\n",
      "%load_ext autoreload\n",
      "%autoreload 2\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "import blackhc.project.script\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BatchBALD Algorithm\n",
    "> and compute BatchBALD scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will implement two helper classes to compute conditional entropies $H[y_i|w]$ and entropies $H[y_i]$.\n",
    "\n",
    "Then, we will implement BatchBALD and then BALD as encore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "from dataclasses import dataclass\n",
    "from typing import List\n",
    "import torch\n",
    "\n",
    "from toma import toma\n",
    "\n",
    "from batchbald_redux import joint_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing conditional entropies and batch entropies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "\n",
    "def compute_conditional_entropy(probs_N_K_C: torch.Tensor) -> torch.Tensor:\n",
    "    N, K, C = probs_N_K_C.shape\n",
    "    \n",
    "    entropies_N = torch.empty(N, dtype=torch.double)\n",
    "    @toma.chunked(probs_N_K_C, 1024)\n",
    "    def compute(probs_n_K_C, start:int, end: int):\n",
    "        entropies_N[start:end].copy_(-torch.sum(probs_n_K_C*torch.log(probs_n_K_C), dim=(1,2))/K)\n",
    "    \n",
    "    return entropies_N\n",
    "\n",
    "\n",
    "def compute_entropy(probs_N_K_C: torch.Tensor) -> torch.Tensor:\n",
    "    N, K, C = probs_N_K_C.shape\n",
    "    \n",
    "    entropies_N = torch.empty(N, dtype=torch.double)\n",
    "    @toma.chunked(probs_N_K_C, 1024)\n",
    "    def compute(probs_n_K_C, start:int, end: int):\n",
    "        mean_probs_N_C = probs_N_K_C.mean(dim=1)        \n",
    "        entropies_N[start:end].copy_(-torch.sum(mean_probs_N_C*torch.log(mean_probs_N_C), dim=1))\n",
    "    \n",
    "    return entropies_N\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example usages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BatchBALD\n",
    "\n",
    "To compute BatchBALD exactly for a candidate batch, we'd have to compute $I[(y_b)_B;w] = H[(y_b)_B] - H[(y_b)_B|w]$.\n",
    "\n",
    "As the $y_b$ are independent given $w$, we can simplify $H[(y_b)_B|w] = \\sum_b H[y_b|w]$.\n",
    "\n",
    "Furthermore, we use a greedy algorithm to build up the candidate batch, so $y_1,\\dots,y_{B-1}$ will stay fixed as we determine $y_{B}$. We compute\n",
    "$H[(y_b)_{B-1}, y_i] - H[y_i|w]$ for each pool element $y_i$ and add the highest scorer as $y_{B}$.\n",
    "\n",
    "We don't utilize the last optimization here in order to compute the actual scores.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In the paper\n",
    "\n",
    "![BatchBALD algorithm in the paper](batchbald_algorithm.png)\n",
    "\n",
    "\n",
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CandidateBatch:\n",
    "    scores: List[float]\n",
    "    indices: List[int]\n",
    "\n",
    "\n",
    "def get_batchbald_batch(probs_N_K_C: torch.Tensor,\n",
    "                        batch_size: int,\n",
    "                        num_samples: int,\n",
    "                        dtype=None,\n",
    "                        device=None) -> CandidateBatch:\n",
    "    N, K, C = probs_N_K_C.shape\n",
    "    \n",
    "    batch_size = min(batch_size, N)\n",
    "    \n",
    "    candidate_indices = []\n",
    "    candidate_scores = []\n",
    "    \n",
    "    if batch_size == 0:\n",
    "        return CandidateBatch(candidate_scores, candidate_indices)\n",
    "\n",
    "    batch_joint_entropy = joint_entropy.DynamicJointEntropy(num_samples,\n",
    "                                                            batch_size - 1,\n",
    "                                                            K,\n",
    "                                                            C,\n",
    "                                                            dtype=dtype,\n",
    "                                                            device=device)\n",
    "\n",
    "    conditional_entropies_N = compute_conditional_entropy(probs_N_K_C)\n",
    "\n",
    "    # We always keep these on the CPU.\n",
    "    scores_N = torch.empty(N, dtype=torch.double)\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        if i > 0:\n",
    "            latest_index = candidate_indices[-1]\n",
    "            batch_joint_entropy.add_variables(\n",
    "                probs_N_K_C[latest_index:latest_index + 1])\n",
    "            shared_conditinal_entropies = conditional_entropies_N[candidate_indices].sum()\n",
    "        else:\n",
    "            shared_conditinal_entropies = 0.\n",
    "            \n",
    "        batch_joint_entropy.compute_batch(probs_N_K_C,\n",
    "                                          output_entropies_B=scores_N)\n",
    "\n",
    "        scores_N -= conditional_entropies_N + shared_conditinal_entropies\n",
    "        scores_N[candidate_indices] = -float('inf')\n",
    "\n",
    "        candidate_score, candidate_index = scores_N.max()\n",
    "\n",
    "        candidate_indices.append(candidate_index.item())\n",
    "        candidate_scores.append(candidate_scores.item())\n",
    "\n",
    "    return CandidateBatch(candidate_scores, candidate_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example usages:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encore: BALD\n",
    "\n",
    "BALD is the same as BatchBALD, except that we pick candidates independently from each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bald_batch(probs_N_K_C: torch.Tensor,\n",
    "                   batch_size: int,\n",
    "                   num_samples: int,\n",
    "                   dtype=None,\n",
    "                   device=None) -> CandidateBatch:\n",
    "    N, K, C = probs_N_K_C.shape\n",
    "    \n",
    "    batch_size = min(batch_size, N)\n",
    "\n",
    "    candidate_indices = []\n",
    "    candidate_scores = []\n",
    "\n",
    "    scores_N = compute_entropy(probs_N_K_C)\n",
    "    scores_N -= compute_conditional_entropy(probs_N_K_C)\n",
    "    \n",
    "    candiate_scores, candidate_indices = torch.topk(scores_N, batch_size)\n",
    "    \n",
    "    return CandidateBatch(candiate_scores.tolist(), candidate_indices.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example usages:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
